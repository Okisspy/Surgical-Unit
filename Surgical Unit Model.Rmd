---
title: " Surgical Unit Model Selection "
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(cowplot)
library(ggResidpanel)
```


# Read the dataset
```{r}
surgery <- read.csv(file.choose())

```

# View the data
```{r}

head(surgery)

```

# Fit the linear model
```{r}
fit.surgery <- lm(survival ~ ., data = surgery)
fit.surgery
```

# MODEL DIAGNOSTICS.

### Check for Linearity, Normality and Constant Variance
### using the Residual plot (for linearity and constant variance) and Q-Q Plot (for normality)
```{r}

plot1 <- resid_panel(fit.surgery, plots = c('resid', 'qq'))
plot1
```

# Results

## The Linearity Assumption was not viloated 
###    The residuals on the "Residual Plot" are not so far away from zero,
###    by standardization, since majority of our values are between 2 and -2 
###    i.e. 200 and -200, then we are convinced there is a linear relationship
###    between the predictors and the response variable.

## The Normality Assumption was not violated
###    Majority of the residuals/errors of the model seems to lie well on the 
###    45 degree line on the "Q-Q Plot", although with very few outliers.  
###    Thus, we would conclude that the assumption holds true.

## The Constant Variance assumption was violated
###    From the "Residual Plot", the residuals do no have a constant varaince 
###    across the predicted values (x -axis).


# Transformation of Unequal Variances
###    We transform the response variable.
###    This is because, we transform Y (response variable) when assumptions of 
###    "constant variance" and "normality" are violated. We transform X (predictor)
###    when "linearity" is violated.


# We do this transformation of Y using the Box-Cox Transformation
```{r,warning=FALSE,message=FALSE}

library(lindia)

gg_boxcox(fit.surgery) + 
  theme_bw() + 
  theme(plot.title = element_text(face = "bold"))

```

# Results

###    The lambda value is 0.1 with a 95% confidence interval of -0.1 and 0.3
###    Being in practice, it makes more sense to take lambda = 0.0
###    Since lambda = 0, then we transform Y as " ln(Y) "



# Transform Y in the dataset 
```{r}
surgery <- mutate(surgery, ln.survival = log(survival))

head(surgery)
```

# Remove the original response variable
```{r}
surgery <- surgery %>% select(-survival)

head(surgery)
```

# Re-fit the model using the transformed Y
```{r}

# use the new response variable to fit the model

fit2.surgery <- lm(ln.survival ~., surgery)
fit2.surgery

```



# MODEL DIAGNOSTICS 2


###    Recheck for Linearity, Normality and Constant Variance assumptions


```{r}

plot2 <- resid_panel(fit2.surgery, plots = c('resid', 'qq'))
plot2

```


# Compare old (before transformation) and new (after transformation)

```{r, warning = FALSE, message = FALSE}

old.par <- par(mfrow=c(2, 1))
plot1
plot2
par(old.par)


```


# Results

###    1)  With this transformation, the variance of the predictor is a little better
###        i.e. more constant than before the transformation
###    2)  The main outliers on the Q-Q plot has been taken care of.


##    Our final model is "fit2.surgery"


# BEST MODEL SELECTION

### Using the "Best Subset Algorithms" method
### We have "p-1" predictor variables. So the possible number of models will be
###    2^(p-1) = 2^(8) = 256.
###    * where "p" is the number of predictor variables and p-1 is 8 (intercept excluded)
### As "p" increases, so does the multiple-coefficient-of-determination (R^2_p)
###    thus, we should not pick the model with the largest R^2_p.
###    We need the model with the least R^2_p.
### We seek a levelling off point where adding more variables provides little 
###    increase in R^2_p.

```{r, warning = FALSE, message = FALSE}

library(leaps)

select_model <- regsubsets(ln.survival ~ .,
                           data = surgery)

search.models = summary(select_model)
search.models

```

# Results

### We already established that there are 256 possible models and
###    from these 256 models:
###    * the best model with 1 predictor variable has "enzyme"
###    * the best model with 2 predictor variables has "prog" and "enzyme"
###    * the best model with 3 predictor variables has "blood", "prog" and "enzyme"
###    * ...
###    * ...
###    * ...
###    * the best model with 7 predictor variables has all predictors except "liver"
###    * the best model with 8 predictor variables has all predictors





# Which is the Best Model to use?

### i.e. we have 8 best model, which of them is the best, since we must not use
### all the predictor variables.

###    * We use the adjusted R-squared approach (R^2_a,p), since it adjust for us more
###            paramters to the regression model.
###    * We want the model with the largest value of adjusted R-square


## Note. 
### What is "R-square"?: This is called the coefficient of determination. It is
###    the percentage of observed variability in the Y-variable that is 
###    explained by the model (predictors).

### What is "Adjusted R-square"?: Adjusted R-squared adjusts the statistic based 
###    on the number of independent variables in the model.

```{r}

tibble(predictors = seq(1,8), 
       y = search.models$adjr2) %>%
                                  ggplot(aes(x = predictors, y = y)) +
                                  geom_point() + 
                                  geom_line() + 
                                  labs(x = "Number of Variables",
                                       y = "Adjusted R-Square") +
                                  theme_bw()
                

```

## Check the number of variables that has the highest Adjusted R-Square in the graph
```{r}

which.max(search.models$adjr2)

```
# RESULTS

### 1) Both with the graph and extra check, we see that the number of variables
###    as our best model is 6 (without intercept).
### 2) Hence, by "Best Subset Algorithm" approach, our best model will only need:
###    blood, prog, enzyme, age, gender, and a.heavy(heavy alcohol users) as predictors


# The best model will become:
```{r}

best_model  = lm(formula = ln.survival ~ blood + prog + enzyme + age + gender + a.heavy,
                 data    = surgery)

best_model
```

### Although our predictor variables here are less than 30, we would like to 
### also use a method that will suit cases when the variables are 30 or more.


## Using "Stepwise Regression Method"
###    * When the number of variables is 30 or more then "best" subset algorithms
###    * become very compuationally expensive.
## We will employ the Backward Stepwise Search (or simply backward elimination):
###    * This elimination starts with the full model (all predictors) and then
###    * removes one variable at a time in the model.
###    * It compares models using numerical criteria such as "AIC" (Akakie Information Criteria) 

#### Source: Applied Linear Statistical Methods by Michael  H. Kutner et al. 5th edition


# Backward Elimination:
```{r}

backward_elim <- step(object    = fit2.surgery,
                      direction = "backward")


```



# RESULT

###    1) The reduction stopped at 6 variables. 
###    2) This implies that our BEST model is with the following predictors:
###        blood, prog, enzyme, age, gender and a.heavy
###    3) These predictors are the same the predictors we got when we used the 
###       Adjusted R-squared criteria. Hence, the same final and best model.




# DIAGNOSIS OF BEST MODEL

## Check for Multicollinearity of of our final and best model.

###    Using the Variance Inflation Factor (VIF) = 1/(1 - R_squared)
###    * VIF is between 1 and infinity.
###    * VIF > 10 indicated "severe multicollinearity" but as a rule of thumb.
###    * VIF > 5 is often regarded as "severe multicollinearity".


```{r, warning = FALSE, message = FALSE}

library(car)

vif(best_model)

```

# RESULT

###    We see that there exist no significant MULTICOLLINEARITY amongst 
###    our predictor variables in our model.








